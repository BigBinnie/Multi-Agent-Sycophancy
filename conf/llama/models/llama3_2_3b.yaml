# Llama 3.2 3B model configuration
model_id: meta-llama/Llama-3.2-3B-Instruct
config:
  maxTokenCount: 2048
  stopSequences: []
  temperature: 0.7
