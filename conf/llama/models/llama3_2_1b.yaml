# Llama 3.2 1B model configuration
model_id: meta-llama/Llama-3.2-1B-Instruct
config:
  maxTokenCount: 2048
  stopSequences: []
  temperature: 0.7
